---
toc: true
layout: post
description: A simple post that helps us to learn SGD
categories: [markdown]
title: Stochastic Gradient Descent
---
This post is inspired by fastai course.
- Starting from example of 3's and 7's prediction, the reason of choosing a good loss function and then a sigmoid function is detailed. A good loss function is used to align with a small change of gradient will lead to the significant changes in terms of prediction. In order to guarantee the prediction in a particular range e.g [0,1], a sigmoid function is being used.   
- Definition of epoch, Stochastic Gradient Descent and mini-batches
- an optimizer
- Putting all the elements together
- Nonlinear element in neural network - Universal Approximatio Theorem
- dead unit problem when the output of nn cause many zero.
